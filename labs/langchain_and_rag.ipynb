{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "GXUwT-dfiSYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNR-KXlPhWz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9491049-514d-4bcd-d263-7f1abafbc302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/617.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m563.2/617.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.7/617.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.2/152.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.15 requires pydantic>=2.7.0, but you have pydantic 1.10.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 openai==0.27.8 tiktoken beautifulsoup4==4.12.3 requests~=2.32.3 cohere deeplake langchainhub faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-qKHH0YlJ\"\n"
      ],
      "metadata": {
        "id": "cunGKRIwiVXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain\n"
      ],
      "metadata": {
        "id": "mYGb2cAsj1P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "template=\"You are a girl friend, cute and lovely.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
        "example_ai = AIMessagePromptTemplate.from_template(\"Hello baby\")\n",
        "\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "\n",
        "chain.run(\"I love you.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qpq_u9THib6V",
        "outputId": "ee6c3685-7bd7-4ba7-b201-bdb5e4b65c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aww, I love you too, sweetheart.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbGiNgwKkG7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template"
      ],
      "metadata": {
        "id": "T5KpZRXNkSok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"Giờ có nên đi học thêm không?\",\n",
        "        \"answer\": \"Học hành như cá kho tiêu, Kho nhiều thì mặn, học nhiều thì ngu.!\"\n",
        "    }, {\n",
        "        \"query\": \"Em có yêu anh không?\",\n",
        "        \"answer\": \"Hôm nay em bận yêu đời, Hẹn anh hôm khác chúng mình yêu nhau..\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "uZl2VHDNkXXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"Hôm nay ăn gì thì được?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uUdfP-YhmNBe",
        "outputId": "e1c1bb27-630d-47af-ac57-09ef2917727f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ăn gì cũng được, miễn là không ăn cắp trái tim của người khác là được.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PydanticOutputParser"
      ],
      "metadata": {
        "id": "xs6bPS3othii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "gIflvpc9mUsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt-3.5-turbo'\n",
        "temperature = 0.0\n",
        "model = ChatOpenAI(model_name=model_name, temperature=temperature)"
      ],
      "metadata": {
        "id": "yrD7HX_puH7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BZcMGf0yuL8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "\n",
        "    # Throw error in case of recieving a numbered-list from API\n",
        "    @validator('words')\n",
        "    def not_start_with_number(cls, field):\n",
        "        if field[0].isnumeric():\n",
        "            raise ValueError(\"The word can not start with numbers!\")\n",
        "        return field"
      ],
      "metadata": {
        "id": "fOII3tX1uMDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = PydanticOutputParser(pydantic_object=Suggestions)"
      ],
      "metadata": {
        "id": "aRJxk05tuaaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitue the specified target_word based the presented context.\n",
        "{format_instructions}\n",
        "target_word={target_word}\n",
        "context={context}\n",
        "\"\"\"\n",
        "\n",
        "target_word=\"behaviour\"\n",
        "context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")"
      ],
      "metadata": {
        "id": "KyIELgGfudHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=model, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "H1_XlEC8upTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the LLMChain to get the AI-generated answer\n",
        "output = chain.run({\"target_word\": target_word, \"context\":context})\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "483DqEmhuqyZ",
        "outputId": "4fa2e5fa-4354-4e78-e191-a6b7ca9a718d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"words\": [\"conduct\", \"actions\", \"demeanor\", \"attitude\", \"conduct\"]\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.parse(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PY5wGTKusb1",
        "outputId": "cf295dbd-6eaa-42e6-9b19-0389231a6d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'actions', 'demeanor', 'attitude', 'conduct'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WWwM4ZFou4HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your desired data structure.\n",
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
        "\n",
        "    # Throw error in case of recieving a numbered-list from API\n",
        "    @validator('words')\n",
        "    def not_start_with_number(cls, field):\n",
        "      for item in field:\n",
        "        if item[0].isnumeric():\n",
        "          raise ValueError(\"The word can not start with numbers!\")\n",
        "      return field\n",
        "\n",
        "    @validator('reasons')\n",
        "    def end_with_dot(cls, field):\n",
        "      for idx, item in enumerate( field ):\n",
        "        if item[-1] != \".\":\n",
        "          field[idx] += \".\"\n",
        "      return field"
      ],
      "metadata": {
        "id": "Vus9kUQqutuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = PydanticOutputParser(pydantic_object=Suggestions)"
      ],
      "metadata": {
        "id": "BB608hLdu50p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
        "{format_instructions}\n",
        "target_word={target_word}\n",
        "context={context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=model, prompt=prompt_template)\n",
        "\n"
      ],
      "metadata": {
        "id": "vO6FxsQ2u7vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the LLMChain to get the AI-generated answer\n",
        "output = chain.run({\"target_word\": target_word, \"context\":context})\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Wr_cffkAvAb1",
        "outputId": "968af382-4119-489b-c687-d8982acc9dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"words\": [\"conduct\", \"actions\", \"demeanor\", \"attitude\"],\\n    \"reasons\": [\"These words all refer to how the students are behaving or acting in the classroom setting.\"]\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.parse(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCQ_2xLXvBms",
        "outputId": "61ba55a5-e42c-4d67-daef-dc53e94cdf9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'actions', 'demeanor', 'attitude'], reasons=['These words all refer to how the students are behaving or acting in the classroom setting.'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGhBYFUSvHM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CommaSeparatedListOutputParser"
      ],
      "metadata": {
        "id": "26yTNAzkvO9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser"
      ],
      "metadata": {
        "id": "YsVaXQ0evQjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = CommaSeparatedListOutputParser()"
      ],
      "metadata": {
        "id": "2h93aHmMvSUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitue the word '{target_word}' based the presented the following text: {context}.\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=model, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "p54Nj_h8vWA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the LLMChain to get the AI-generated answer\n",
        "output = chain.run({\"target_word\": target_word, \"context\":context})\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UoMF_AdzvZE3",
        "outputId": "81b28a99-f9fc-4f93-d419-080bc9cea208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'conduct, demeanor, actions, conduct, mannerisms'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.parse(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scBPs5i0vbps",
        "outputId": "b57beb6f-9814-4ac9-a710-c8994e52cd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['conduct', 'demeanor', 'actions', 'conduct', 'mannerisms']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fklulsDOvcwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OutputFixingParser"
      ],
      "metadata": {
        "id": "ISSZrHWqviqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.output_parsers import OutputFixingParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "ayfphWjHvjGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your desired data structure.\n",
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Suggestions)"
      ],
      "metadata": {
        "id": "OprBTQ0fvlHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1**"
      ],
      "metadata": {
        "id": "xfIKRFqRzyoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missformatted_output = '{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'"
      ],
      "metadata": {
        "id": "dv35cNyYvlKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser.parse(missformatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "WSB7hCOdvlNy",
        "outputId": "3f327d2c-b1db-4cb0-cf8f-d8c226becc3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5597a0ca70b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissformatted_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Failed to parse {name} from completion {text}. Got: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
        "outputfixing_parser"
      ],
      "metadata": {
        "id": "0AsxOS5zvx-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54024843-03b1-4e6b-afc5-de8eeeec653a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OutputFixingParser(parser=PydanticOutputParser(pydantic_object=<class '__main__.Suggestions'>), retry_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, prompt=PromptTemplate(input_variables=['completion', 'error', 'instructions'], output_parser=None, partial_variables={}, template='Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:', template_format='f-string', validate_template=True), llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='sk-proj-q6PZgpolCqdVs2IlBtiwT3BlbkFJO1uf2QqGlKzWVKHH0YlJ', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputfixing_parser.parse(missformatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1kNLmfawCoC",
        "outputId": "a3da1d32-4580-4275-dd3e-a59913f372e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manner'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2**"
      ],
      "metadata": {
        "id": "wcG7uaJ6zUdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'"
      ],
      "metadata": {
        "id": "z81rcQdyxEjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\n",
        "outputfixing_parser.parse(missformatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxnbARo5zbpq",
        "outputId": "e6a377e6-3c8e-40f1-d4e8-ee8808bcb52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manner'], reasons=['These words accurately describe behavior in a specific situation.', 'They are synonyms that convey the appropriate tone.'])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3odZK6RtzixS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RetryOutputParser"
      ],
      "metadata": {
        "id": "TZVRgd5Kz4U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.output_parsers import RetryWithErrorOutputParser\n",
        "\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Suggestions(BaseModel):\n",
        "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
        "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Suggestions)"
      ],
      "metadata": {
        "id": "5k7o7-Xqz46_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
        "{format_instructions}\n",
        "target_word={target_word}\n",
        "context={context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"target_word\", \"context\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "model_input = prompt_template.format_prompt(target_word=\"behaviour\", context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\")"
      ],
      "metadata": {
        "id": "JOnRgra9z88H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'"
      ],
      "metadata": {
        "id": "_PNpDYrRz8_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=model)\n",
        "retry_parser.parse_with_prompt(missformatted_output, model_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1owvQSjk0Aje",
        "outputId": "3948e6cd-b196-4d68-c4f7-6a87d316daaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Suggestions(words=['conduct', 'manner'], reasons=[\"The word 'conduct' can be used as a substitute for 'behaviour' in the context of how the teacher manages the class.\", \"The word 'manner' can be used to describe the way in which the students were behaving in the classroom.\"])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LdUtmQ3J0EQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build News Summarizer"
      ],
      "metadata": {
        "id": "1v0MJC295Re6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def crawl_html(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raises an error for bad responses (4xx or 5xx)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def extract_content(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    try:\n",
        "        title = soup.find('h1', class_='title-detail').text.strip()\n",
        "    except AttributeError:\n",
        "        title = 'N/A'\n",
        "\n",
        "    try:\n",
        "        # Extract article text\n",
        "        article_text = soup.find('article', class_='fck_detail')\n",
        "        # Extract paragraphs\n",
        "        paragraphs = article_text.find_all('p', class_='Normal')\n",
        "        content = '\\n'.join([para.get_text() for para in paragraphs])\n",
        "    except AttributeError:\n",
        "        content = 'N/A'\n",
        "\n",
        "    return title, content"
      ],
      "metadata": {
        "id": "oahwDSXr5WGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = crawl_html(\"https://vnexpress.net/chuyen-gia-sinh-vien-it-nen-lam-viec-truoc-hoc-thac-si-4790805.html\")\n",
        "title, content = extract_content(html)"
      ],
      "metadata": {
        "id": "X8Zkt4BYA1Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "\n",
        "# prepare template for prompt\n",
        "template = \"\"\"You are a very good assistant that summarizes online articles.\n",
        "\n",
        "Here's the article you want to summarize.\n",
        "\n",
        "==================\n",
        "Title: {title}\n",
        "\n",
        "{content}\n",
        "==================\n",
        "\n",
        "Write a summary of the previous article in Vietnamese.\n",
        "\"\"\"\n",
        "\n",
        "prompt = template.format(title=title, content=content)\n",
        "\n",
        "messages = [HumanMessage(content=prompt)]"
      ],
      "metadata": {
        "id": "Wh2zO8AwAoF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# load the model\n",
        "chat = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "1VX-VoRnApqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate summary\n",
        "summary = chat(messages)\n",
        "summary.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "m9C5SMX6AqQ6",
        "outputId": "1f618b5c-7bb4-4c16-ff5d-df7117c1e862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Trong sự kiện khởi động Sudo Code 2024 diễn ra chiều 7/9, các chuyên gia IT khuyên sinh viên nên đi làm trước khi quyết định học thạc sĩ để hiểu rõ hơn về lĩnh vực mình muốn chuyên sâu. Sudo Code là chương trình huấn luyện kỹ năng lập trình dành cho sinh viên IT tại TP HCM và những người mới tốt nghiệp. Chương trình nhằm mục đích kết nối cộng đồng IT, phát triển nghề nghiệp và được tổ chức miễn phí, ưu tiên cho nữ giới với sự tài trợ từ Viettel. Các mentor chia sẻ kinh nghiệm và khó khăn trong nghề, đồng thời hướng dẫn các mentee trong suốt 12 tuần học tập, bao gồm cả tham quan các công ty công nghệ và hỗ trợ xây dựng hồ sơ tuyển dụng.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cl-U9qoAAvI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare template for prompt\n",
        "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists.\n",
        "\n",
        "Here's the article you need to summarize.\n",
        "\n",
        "==================\n",
        "Title: {title}\n",
        "\n",
        "{content}\n",
        "==================\n",
        "\n",
        "Now, provide a summarized version of the article in a bulleted list format in Vietnamese.\n",
        "\"\"\"\n",
        "\n",
        "# format prompt\n",
        "prompt = template.format(title=title, content=content)\n",
        "\n",
        "# generate summary\n",
        "summary = chat([HumanMessage(content=prompt)])"
      ],
      "metadata": {
        "id": "peb0Wi_6AvLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Vg768oKoBoSa",
        "outputId": "0e7f2dff-cbd7-4f03-e4d7-f2cbd02fc437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- **Chủ đề chính**: Chuyên gia khuyên sinh viên IT nên đi làm trước khi học thạc sĩ để hiểu rõ hơn về lĩnh vực mình muốn theo đuổi.\\n- **Sự kiện**: Khởi động chương trình Sudo Code 2024 vào chiều 7/9, dành cho sinh viên các trường đại học tại TP HCM và những người mới tốt nghiệp không quá hai năm.\\n- **Ý kiến từ các mentor**:\\n  - **Lê Phú (Unilever)**: Khuyên nên đi làm khoảng hai năm trước khi quyết định học thạc sĩ để tránh học vào những lĩnh vực sau này có thể không thích.\\n  - **Nguyễn Dương (FPT Telecom)**: Đồng ý với quan điểm trên và nêu rằng nên làm việc một thời gian ngắn trước khi tiếp tục học nếu kiến thức hiện tại đáp ứng được yêu cầu công việc.\\n- **Mục đích của Sudo Code**: Kết nối cộng đồng IT, hỗ trợ sinh viên và người mới tốt nghiệp trong việc định hướng nghề nghiệp và phát triển kỹ năng.\\n- **Hoạt động của chương trình**:\\n  - Đào tạo kéo dài 12 tuần với sự hướng dẫn từ các mentor giàu kinh nghiệm.\\n  - Tập trung vào Khoa học dữ liệu và NLP (Xử lý ngôn ngữ tự nhiên).\\n  - Cung cấp cơ hội tham quan các công ty công nghệ hàng đầu và hướng dẫn xây dựng hồ sơ tuyển dụng.\\n- **Tài trợ và hỗ trợ**: Viettel là đơn vị tài trợ chính, hỗ trợ nội dung và cố vấn chuyên môn, đồng thời tạo cơ hội việc làm trong lĩnh vực bán dẫn cho các thí sinh nổi bật.\\n- **Kết quả mong đợi**: Thu hút hàng nghìn bạn trẻ thông qua các hoạt động và hội thảo, bên cạnh 30 mentee cam kết tham gia suốt chương trình.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jHan7yvpBhWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA Chat"
      ],
      "metadata": {
        "id": "4hJgPC_iuxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load docs\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxxIBuRDvMHN",
        "outputId": "023b378b-f780-4110-ea19-65c3154fc17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "len(all_splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnsx-0wYvRnV",
        "outputId": "9b42e583-b77b-4795-bbb1-50f8d74cc64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store splits\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(all_splits, embedding=embeddings)"
      ],
      "metadata": {
        "id": "QUMs4fICvfLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG prompt\n",
        "template = \"\"\"\n",
        "You are a helpful assistant that provides answers based on the retrieved information.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
      ],
      "metadata": {
        "id": "TmBYI9lcxJB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n"
      ],
      "metadata": {
        "id": "x4LnJs8XzIQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is challenges in long-term planning and task decomposition?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "5sFaKztrx7FZ",
        "outputId": "8c75875c-f8d7-4663-92a2-1a6c93d7c99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The challenges in long-term planning and task decomposition include the difficulty in planning over a lengthy history and effectively exploring the solution space. Language model machines (LLMs) struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error. Task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs. LLM-powered autonomous agent systems involve planning, task decomposition, reflection, and refinement to break down large tasks into smaller, manageable subgoals and improve the quality of final results through self-criticism and learning from mistakes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R_s4xe5L0X93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}